{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate linear\n",
    "\n",
    "Multivariate linear regression is a statistical method used for modeling the relationship between multiple independent variables and a single dependent variable.\n",
    "In contrast to [univariate linear regression](../univariate-linear), which involves only one independent variable, multivariate linear regression considers several simultaneously.\n",
    "The goal is to create a linear equation that best fits the observed data, allowing for predictions or explanations of the dependent variable based on the values of the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, let's load our CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"https://gitlab.com/oasci/courses/pitt/biosc1540-2024s/-/raw/main/biosc1540/files/csv/advertising-data.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of multivariate regression, we collect all of our independent variables in one dataframe called `df_features` and our dependent variable in `df_targets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"Product_Sold\"\n",
    "\n",
    "df_targets = df[target_column]\n",
    "df_features = df.drop(columns=[target_column], inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to convert the dataframe to NumPy arrays and reshape the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = df_targets.to_numpy().reshape(-1, 1)\n",
    "features = df_features.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear\n",
    "\n",
    "Linear regression is a linear modeling algorithm used in machine learning for regression tasks.\n",
    "Its purpose is to model the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation to the observed data. The linear equation is of the form:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "-   $y$ is the dependent variable (target),\n",
    "-   $\\beta_0$​ is the intercept (constant),\n",
    "-   $\\beta_1, \\beta_2, \\ldots, \\beta_n$​ are the coefficients associated with each feature $x_1, x_2, \\ldots, x_n$​​.\n",
    "\n",
    "Linear regression is commonly used for predicting a continuous variable (regression task) based on one or more input features.\n",
    "It assumes a linear relationship between the independent and dependent variables.\n",
    "\n",
    "The accuracy of a linear regression model is typically measured using metrics such as R-squared ($R^2$), Mean Squared Error (MSE), or Mean Absolute Error (MAE).\n",
    "$R^2$ represents the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "A higher $R^2$ indicates a better fit.\n",
    "\n",
    "Linear regression is widely used in various fields, including economics, finance, biology, and marketing, for modeling and analyzing the relationships between variables.\n",
    "In the context of your example, the company is using linear regression to understand the impact of advertising spending on product sales, helping them make informed decisions about adjusting ad budgets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n",
    "\n",
    "The goal of linear regression is to find the values of the coefficients that minimize the difference between the predicted values and the actual values in the training data. This is often done using a method called Ordinary Least Squares (OLS).\n",
    "Specifically, the objective function to be minimized is the sum of squared differences between the predicted values ($y_i$) and the actual values ($\\hat{y}$):\n",
    "\n",
    "$$\n",
    "\\text{minimize} \\sum \\left( y_i - \\hat{y}_i \\right)^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "Linear regression is a powerful and widely used statistical method for modeling the relationship between variables.\n",
    "However, like any modeling approach, it has its limitations.\n",
    "Here, we explore the constraints and considerations associated with linear regression.\n",
    "\n",
    "**Assumption of Linearity**\n",
    "\n",
    "Linear regression assumes a linear relationship between the independent and dependent variables. The model is designed to capture linear patterns, and deviations from linearity can lead to inaccurate predictions.\n",
    "\n",
    "**Sensitivity to Outliers**\n",
    "\n",
    "Linear regression is sensitive to outliers, which are extreme observations that can disproportionately influence the model. Outliers can skew coefficient estimates and compromise the accuracy of predictions.\n",
    "\n",
    "**Assumption of Independence**\n",
    "\n",
    "The independence of residuals is a critical assumption in linear regression. If residuals exhibit autocorrelation, where the values are not independent, it can violate the model's assumptions and impact the reliability of results.\n",
    "\n",
    "**Multicollinearity**\n",
    "\n",
    "Multicollinearity arises when independent variables are highly correlated, leading to instability in coefficient estimates. This phenomenon complicates the interpretation of individual variable impacts and poses challenges in identifying true predictors.\n",
    "\n",
    "**Assumption of Homoscedasticity**\n",
    "\n",
    "Homoscedasticity, or constant variance of residuals across different levels of the independent variable, is assumed in linear regression. Heteroscedasticity can introduce bias in standard errors and affect hypothesis testing.\n",
    "\n",
    "**Limited to Linear Relationships**\n",
    "\n",
    "Linear regression is limited to capturing linear relationships between variables. When faced with nonlinear patterns, the model may fail to accurately represent the underlying dynamics.\n",
    "\n",
    "**Influence of Scaling**\n",
    "\n",
    "The scale of variables can impact linear regression coefficients. Changes in variable scale can alter their perceived impact on the model, and comparisons between coefficients may lose meaning.\n",
    "\n",
    "**No Variable Selection**\n",
    "\n",
    "Linear regression includes all features in the model, irrespective of their relevance. Lack of automatic variable selection can lead to overfitting if irrelevant features are incorporated.\n",
    "\n",
    "**Not Robust to Violations of Assumptions**\n",
    "\n",
    "Violations of linear regression assumptions, such as nonlinearity or heteroscedasticity, can result in biased estimates and unreliable predictions. The model's performance is contingent on meeting these assumptions.\n",
    "\n",
    "**Correlation vs. Causation**\n",
    "\n",
    "While linear regression establishes correlations between variables, it does not imply causation. Associations observed may be coincidental or influenced by unobserved factors, necessitating caution in drawing causal inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "#### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.97147823 2.79786525 1.59446751 2.43283307 1.40693022 3.91183385]]\n",
      "[36.65524744]\n",
      "0.9401750192922066\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X=features, y=targets)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)\n",
    "print(reg.score(X=features, y=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients (`reg.coef_`) represent the weights assigned to each advertising channel (TV, Billboards, Google Ads, Social Media, Influencer Marketing, Affiliate Marketing) in predicting product sales.\n",
    "These coefficients indicate the estimated change in product sales for a one-unit increase in each respective advertising channel, holding other variables constant.\n",
    "\n",
    "The intercept (`reg.intercept_`) is the constant term in the linear equation.\n",
    "The intercept represents the estimated product sales when all advertising spending is zero.\n",
    "\n",
    "The R-squared value (`reg.score(X=features, y=targets)`) indicates the proportion of the variance in product sales that the advertising spending variables can explain. In this case, the model explains approximately 94% of the variability in product sales.\n",
    "\n",
    "In helping the company, you can use these coefficients to guide them on how each advertising channel contributes to product sales.\n",
    "For example, higher coefficients suggest a stronger positive impact on sales.\n",
    "Adjusting ad budgets based on these coefficients could optimize their advertising strategy for higher sales.\n",
    "Remember that correlation does not imply causation, so further analysis and experimentation may be needed to validate the findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge\n",
    "\n",
    "Ridge regression, also known as Tikhonov regularization or $L2$ regularization, is a linear regression technique that introduces a regularization term to the linear regression objective function.\n",
    "The primary goal of ridge regression is to address the issue of multicollinearity in linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n",
    "\n",
    "\n",
    "In ridge regression, a regularization term is added to this objective function.\n",
    "The new objective function becomes:\n",
    "\n",
    "$$\n",
    "\\text{minimize} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "-   $n$ is the number of observations.\n",
    "-   $p$ is the number of predictors (features).\n",
    "-   $y_i$​ is the actual value of the dependent variable for the ii-th observation.\n",
    "-   $\\hat{y}_i$​ is the predicted value of the dependent variable for the ii-th observation.\n",
    "-   $\\alpha$ is the regularization parameter, controlling the strength of regularization.\n",
    "-   $\\beta_j$ represents the coefficients of the linear regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "\n",
    "**Not Suitable for Feature Selection**\n",
    "\n",
    "One of the primary limitations of ridge regression lies in its inability to perform feature selection.\n",
    "Unlike some other regularization techniques, ridge regression retains all features in the model, making it less suitable for scenarios where variable selection is a critical requirement.\n",
    "\n",
    "**Interpretability Challenges**\n",
    "\n",
    "While ridge regression addresses multicollinearity, it does so at the expense of straightforward interpretability.\n",
    "The regularization term introduces a level of complexity to coefficient interpretation, as the coefficients are shrunk towards zero.\n",
    "This departure from the clear interpretability of standard linear regression should be acknowledged.\n",
    "\n",
    "**Dependency on Scaling**\n",
    "\n",
    "The performance of ridge regression is influenced by the scale of the variables.\n",
    "Rescaling or standardizing the features is often necessary to ensure effective regularization.\n",
    "Failure to do so can result in unequal penalization of coefficients, impacting the model's stability.\n",
    "\n",
    "**No Sparsity in Coefficients**\n",
    "\n",
    "Unlike some regularization methods, such as LASSO (L1 regularization), ridge regression does not lead to exact zero coefficients.\n",
    "It shrinks coefficients towards zero but retains all features in the model.\n",
    "If sparsity is a critical consideration, alternative regularization methods may be more appropriate.\n",
    "\n",
    "**Model Complexity**\n",
    "\n",
    "Ridge regression introduces a level of model complexity, with the choice of the regularization parameter ($\\alpha$) playing a pivotal role.\n",
    "Selecting an optimal αα value often involves techniques like cross-validation, adding an additional layer of complexity to model tuning.\n",
    "\n",
    "**Assumption of Linearity**\n",
    "\n",
    "Similar to standard linear regression, ridge regression assumes a linear relationship between the independent and dependent variables.\n",
    "If the true relationship is significantly nonlinear, ridge regression may not capture the underlying patterns accurately.\n",
    "\n",
    "**Limited Handling of Multicollinearity**\n",
    "\n",
    "While ridge regression is effective in mitigating multicollinearity, it may not completely eliminate the issue.\n",
    "In cases of severe multicollinearity, additional techniques or data preprocessing methods may be necessary.\n",
    "\n",
    "**Sensitivity to Outliers**\n",
    "\n",
    "Ridge regression exhibits reduced sensitivity to outliers compared to standard linear regression, yet extreme values can still influence the regularization process and impact the resulting coefficients.\n",
    "Practitioners should exercise caution in the presence of outliers.\n",
    "\n",
    "**Not a Panacea**\n",
    "\n",
    "It is crucial to recognize that ridge regression is not a universal solution.\n",
    "Its effectiveness depends on the specific characteristics of the data and the objectives of the analysis.\n",
    "Careful consideration of alternative regularization techniques may be warranted in certain scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "#### sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.97147817 2.79786515 1.59446745 2.43283298 1.40693016 3.9118337 ]]\n",
      "[36.65551041]\n",
      "0.9401750192922053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "reg = Ridge(alpha=1.0)\n",
    "reg.fit(X=features, y=targets)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)\n",
    "print(reg.score(X=features, y=targets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biosc1540-2024s-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
