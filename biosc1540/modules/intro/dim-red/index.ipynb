{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "Dimensionality reduction involves decreasing the number of features within a dataset.\n",
    "Dealing with an excessive number of variables, also known as features, is a common challenge in machine learning tasks such as regression or classification.\n",
    "The greater the number of features, the more challenging it becomes to model them&mdash;this phenomenon is referred to as the curse of dimensionality.\n",
    "\n",
    "Moreover, some features may be redundant, introducing unnecessary noise to the dataset.\n",
    "Including these in the training data does not contribute meaningfully.\n",
    "This is where the reduction of the feature space becomes crucial.\n",
    "\n",
    "The dimensionality reduction shifts data from a high-dimensional feature space to a lower-dimensional one.\n",
    "It is crucial to ensure that, concurrently, meaningful properties inherent in the data are retained throughout this transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of Dimensionality\n",
    "\n",
    "In the world of machine learning and deep learning, algorithms need a lot of data to learn patterns and representations effectively.\n",
    "However, when this data has a ton of features, it can lead to something called the curse of dimensionality.\n",
    "\n",
    "For example, we can look at a dataset of pKa values and numerous corresponding molecular descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CSV_PATH_ALL = \"https://gitlab.com/oasci/courses/pitt/biosc1540-2024s/-/raw/main/biosc1540/files/csv/pka/pka_with_desc.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many columns we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1706, 212)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we have 1706 rows of 212 columns.\n",
    "If you take a look at the columns (shown below), you will see \"SMILES\" and \"pka_value\" that are not features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SMILES', 'pka_value', 'MaxAbsEStateIndex', 'MaxEStateIndex',\n",
      "       'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt',\n",
      "       'HeavyAtomMolWt',\n",
      "       ...\n",
      "       'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene',\n",
      "       'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene',\n",
      "       'fr_unbrch_alkane', 'fr_urea'],\n",
      "      dtype='object', length=212)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 210 features&mdash;that's a lot of features.\n",
    "(This is a [flex tape reference](https://www.youtube.com/watch?v=JZLAHGfznlY).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsity\n",
    "\n",
    "This curse suggests that as we try to estimate a function accurately, the number of features or dimensions needed for the estimation grows exponentially.\n",
    "This becomes especially tricky with big data, which tends to be more sparse.\n",
    "\n",
    "Now, sparsity in data means that many features have a value of zero (not that the value is missing).\n",
    "Having lots of sparse features increases space and computational complexity.\n",
    "When data is sparse, it's hard to cluster observations or samples in the training dataset.\n",
    "\n",
    "Let's see which columns of ours are sparse by computing the sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sparsity for each column\n",
    "sparsity_per_column = (df == 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression `(df == 0).mean()` in Python, when applied to a Pandas DataFrame, performs the following steps:\n",
    "\n",
    "-   `df == 0`: This creates a boolean DataFrame where each element is `True` if the corresponding element in `df` is equal to `0`, and `False` otherwise.\n",
    "-   `(df == 0).mean()`: After obtaining the boolean DataFrame, `.mean()` is applied.\n",
    "    This calculates the mean (average) along each column.\n",
    "    Since `True` is treated as `1` and `False` as `0` when calculating the mean, this effectively gives the percentage of elements in each column that are equal to `0`.\n",
    "\n",
    "In simpler terms, `(df == 0).mean()` computes the percentage of zeros in each column of the DataFrame.\n",
    "This can be useful for identifying columns where a large proportion of the values are zero, which is a characteristic of sparse columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Columns:\n",
      "(82,)\n",
      "Index(['NumRadicalElectrons', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA4',\n",
      "       'PEOE_VSA5', 'SMR_VSA2', 'SMR_VSA8', 'SlogP_VSA7', 'SlogP_VSA9',\n",
      "       'EState_VSA11', 'NumAliphaticCarbocycles', 'NumSaturatedCarbocycles',\n",
      "       'NumSaturatedHeterocycles', 'NumSaturatedRings', 'fr_Al_COO',\n",
      "       'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_Ar_COO', 'fr_Ar_NH', 'fr_Ar_OH',\n",
      "       'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_N_O', 'fr_Ndealkylation1',\n",
      "       'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde',\n",
      "       'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide',\n",
      "       'fr_amidine', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzodiazepine',\n",
      "       'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_furan',\n",
      "       'fr_guanido', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide',\n",
      "       'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss',\n",
      "       'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile',\n",
      "       'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso',\n",
      "       'fr_oxazole', 'fr_oxime', 'fr_phenol', 'fr_phenol_noOrthoHbond',\n",
      "       'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine',\n",
      "       'fr_priamide', 'fr_prisulfonamd', 'fr_quatN', 'fr_sulfide',\n",
      "       'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole',\n",
      "       'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane',\n",
      "       'fr_urea'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print or filter columns with sparsity below a certain threshold\n",
    "threshold = 0.90\n",
    "sparse_columns = sparsity_per_column[sparsity_per_column > threshold].index\n",
    "\n",
    "print(\"Sparse Columns:\")\n",
    "print(sparse_columns.shape)\n",
    "print(sparse_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eighty-two columns in our dataframe are mostly just zero and probably would contribute little to any model.\n",
    "Note that this does not consider if columns are equal to 1; however, this would require more complicated analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing the number of dimensions\n",
    "\n",
    "High-dimensional data can overlook key relationships.\n",
    "Meaningful and non-redundant data; however, allows similar data points to come together and cluster in statistically significant regions.\n",
    "\n",
    "Problems with high-dimensional data include:\n",
    "\n",
    "- risk of overfitting the machine learning model;\n",
    "- difficulty in clustering similar features;\n",
    "- increased space and computational time complexity.\n",
    "\n",
    "On the flip side, non-sparse or dense data has non-zero features that contain meaningful and non-redundant information.\n",
    "\n",
    "To combat the curse of dimensionality, techniques like dimensionality reduction come into play.\n",
    "These methods are handy for transforming sparse features into dense ones, cleaning up the data, and extracting relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition algorithms\n",
    "\n",
    "Decomposing signals into components is a technique used to extract meaningful information from data.\n",
    "It is a process of breaking down a signal into its constituent parts, which can be used to understand the underlying structure of the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA (KPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold learning algorithms\n",
    "\n",
    "Manifold learning is a set of techniques used to simplify complex data by reducing its dimensionality.\n",
    "The main goal is to make it easier to understand and visualize the information.\n",
    "Imagine you have a lot of data with 4 or more dimensions, it's hard to picture or make sense of it all at once.\n",
    "Manifold learning helps by projecting this data onto simpler, lower-dimensional structures called manifolds.\n",
    "\n",
    "The idea behind manifold learning is that, in many cases, the data might seem more complex than it actually is.\n",
    "High-dimensional data can be challenging to visualize and understand.\n",
    "Manifold learning steps in to simplify this by creating a lower-dimensional representation of the data.\n",
    "It's like trying to capture the essential features of the data in a more manageable form.\n",
    "\n",
    "Think of manifold learning as a way to improve upon traditional methods like PCA (Principal Component Analysis), which work well for linear structures but might struggle with more complex, non-linear patterns in the data.\n",
    "Manifold learning is like extending these methods to better handle the intricacies and relationships in the data, making it more accessible and interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Manifold Approximation and Projection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminant Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biosc1540-2024s-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
